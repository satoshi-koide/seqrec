import torch
from transformers import EvalPrediction
from transformers.utils import is_flash_attn_2_available

def get_optimal_attention_config():
    """
    ç¾åœ¨ã®ç’°å¢ƒã§åˆ©ç”¨å¯èƒ½ãªæœ€é€Ÿã® Attention å®Ÿè£…ã¨ãƒ‡ãƒ¼ã‚¿å‹ã‚’è¿”ã—ã¾ã™ã€‚
    å„ªå…ˆé †ä½: Flash Attention 2 > SDPA (PyTorch Native) > Eager (Default)
    """
    
    # 1. Flash Attention 2 ãŒä½¿ãˆã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    # (ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ãŠã‚Šã€ã‹ã¤GPUãŒAmpereä»¥ä¸Šã§ã‚ã‚‹ã“ã¨)
    if is_flash_attn_2_available() and torch.cuda.is_available():
        print("ğŸš€ Using Flash Attention 2")
        return {
            "attn_implementation": "flash_attention_2",
            # FA2ã¯ fp16 ã‹ bf16 ãŒå¿…é ˆã€‚
            # GPUãŒ bf16 å¯¾å¿œãªã‚‰ bf16 (æ•°å€¤å®‰å®šæ€§ãŒé«˜ã„)ã€ãã†ã§ãªã‘ã‚Œã° fp16
            "torch_dtype": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        }
    
    # 2. PyTorch 2.0 ä»¥é™ãªã‚‰ SDPA (Scaled Dot Product Attention) ã‚’ä½¿ã†
    # ã“ã‚Œã¯T4ã‚„V100ã§ã‚‚å‹•ä½œã—ã€ãã“ãã“é€Ÿã„
    elif hasattr(torch.nn.functional, "scaled_dot_product_attention"):
        print("âš¡ Using PyTorch SDPA (Scaled Dot Product Attention)")
        return {
            "attn_implementation": "sdpa",
            "torch_dtype": torch.float16, # SDPAã‚‚fp16æ¨å¥¨
        }
    
    # 3. ãã‚Œä»¥å¤– (å¤ã„PyTorchãªã©)
    else:
        print("ğŸ¢ Using Default Attention (Eager)")
        return {
            "attn_implementation": "eager",
            "torch_dtype": torch.float32,
        }